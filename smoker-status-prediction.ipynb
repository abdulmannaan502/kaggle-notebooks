{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install catboost --quiet\n!pip install xgboost --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:25:14.502288Z","iopub.execute_input":"2025-12-04T08:25:14.503465Z","iopub.status.idle":"2025-12-04T08:25:22.698673Z","shell.execute_reply.started":"2025-12-04T08:25:14.503398Z","shell.execute_reply":"2025-12-04T08:25:22.697682Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n# =========================================================\n# 1. LOAD DATA\n# =========================================================\ntrain = pd.read_csv(\"/kaggle/input/smoker-status-prediction-biosignal/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/smoker-status-prediction-biosignal/test.csv\")\n\ny = train[\"smoking\"]\nX = train.drop(columns=[\"id\", \"smoking\"])\nX_test = test.drop(columns=[\"id\"])\n\n# =========================================================\n# 2. FEATURE ENGINEERING \n# =========================================================\n\ndef add_features(df):\n    df[\"BMI\"] = df[\"weight(kg)\"] / (df[\"height(cm)\"]/100)**2\n    df[\"pulse_pressure\"] = df[\"systolic\"] - df[\"relaxation\"]\n    df[\"ldl_hdl_ratio\"] = df[\"LDL\"] / df[\"HDL\"]\n    df[\"chol_hdl_ratio\"] = df[\"Cholesterol\"] / df[\"HDL\"]\n    df[\"trig_hdl_ratio\"] = df[\"triglyceride\"] / df[\"HDL\"]\n    df[\"liver_ratio\"] = df[\"AST\"] / df[\"ALT\"]\n    df[\"eyesight_ratio\"] = df[\"eyesight(left)\"] / df[\"eyesight(right)\"]\n    df[\"hearing_ratio\"] = df[\"hearing(left)\"] / df[\"hearing(right)\"]\n    df[\"blood_health_score\"] = df[\"HDL\"] - df[\"LDL\"] - df[\"Cholesterol\"]\n    df[\"liver_stress\"] = df[\"Gtp\"] + df[\"ALT\"] + df[\"AST\"]\n    df[\"blood_pressure_score\"] = df[\"systolic\"] + df[\"relaxation\"]\n    return df\n\nX = add_features(X)\nX_test = add_features(X_test)\n\n# =========================================================\n# 3. MODEL DEFINITIONS (NO EARLY STOPPING)\n# =========================================================\n\nlgb_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 40,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"seed\": 42\n}\n\nxgb_model = XGBClassifier(\n    n_estimators=1500,\n    learning_rate=0.03,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    max_depth=6,\n    eval_metric=\"auc\",\n    tree_method=\"hist\"\n)\n\ncat_model = CatBoostClassifier(\n    iterations=2000,\n    learning_rate=0.03,\n    depth=6,\n    loss_function=\"Logloss\",\n    verbose=0\n)\n\n# =========================================================\n# 4. STRATIFIED KFOLD + 3-MODEL ENSEMBLE\n# =========================================================\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\noof_lgb = np.zeros(len(train))\noof_xgb = np.zeros(len(train))\noof_cat = np.zeros(len(train))\n\npred_lgb = np.zeros(len(test))\npred_xgb = np.zeros(len(test))\npred_cat = np.zeros(len(test))\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X, y)):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n    \n    # LightGBM\n    lgb_train = lgb.Dataset(X_tr, y_tr)\n    lgb_val   = lgb.Dataset(X_val, y_val)\n\n    lgb_model = lgb.train(\n        lgb_params,\n        lgb_train,\n        num_boost_round=1200,  # no early stopping\n        valid_sets=[lgb_val],\n    )\n\n    oof_lgb[val_idx] = lgb_model.predict(X_val)\n    pred_lgb += lgb_model.predict(X_test) / skf.n_splits\n\n    # XGBoost\n    xgb_model.fit(X_tr, y_tr)\n    oof_xgb[val_idx] = xgb_model.predict_proba(X_val)[:, 1]\n    pred_xgb += xgb_model.predict_proba(X_test)[:, 1] / skf.n_splits\n\n    # CatBoost\n    cat_model.fit(X_tr, y_tr)\n    oof_cat[val_idx] = cat_model.predict_proba(X_val)[:, 1]\n    pred_cat += cat_model.predict_proba(X_test)[:, 1] / skf.n_splits\n\n    print(f\"Fold {fold} AUCs:\",\n          roc_auc_score(y_val, oof_lgb[val_idx]),\n          roc_auc_score(y_val, oof_xgb[val_idx]),\n          roc_auc_score(y_val, oof_cat[val_idx])\n         )\n\n# =========================================================\n# 5. FINAL ENSEMBLE (weighted)\n# =========================================================\n\n# weights tuned for medical datasets\nfinal_oof = (\n    0.40 * oof_lgb +\n    0.35 * oof_cat +\n    0.25 * oof_xgb\n)\n\nfinal_preds = (\n    0.40 * pred_lgb +\n    0.35 * pred_cat +\n    0.25 * pred_xgb\n)\n\nprint(\"ENSEMBLE AUC:\", roc_auc_score(y, final_oof))\n\n# =========================================================\n# 6. SUBMISSION\n# =========================================================\nsubmission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"smoking\": final_preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"submission.csv saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T08:31:10.853482Z","iopub.execute_input":"2025-12-04T08:31:10.854434Z","iopub.status.idle":"2025-12-04T08:32:51.315459Z","shell.execute_reply.started":"2025-12-04T08:31:10.854369Z","shell.execute_reply":"2025-12-04T08:32:51.314407Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 4393, number of negative: 7607\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3159\n[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.366083 -> initscore=-0.549057\n[LightGBM] [Info] Start training from score -0.549057\nFold 0 AUCs: 0.875323987805853 0.8772534354075413 0.8797558298658961\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 4393, number of negative: 7607\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002602 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3183\n[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.366083 -> initscore=-0.549057\n[LightGBM] [Info] Start training from score -0.549057\nFold 1 AUCs: 0.8846242028643514 0.8880025311135991 0.883269616728708\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 4394, number of negative: 7606\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3176\n[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.366167 -> initscore=-0.548697\n[LightGBM] [Info] Start training from score -0.548697\nFold 2 AUCs: 0.8852640974221364 0.8868054717591874 0.8887227326618132\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 4394, number of negative: 7606\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002564 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3163\n[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.366167 -> initscore=-0.548697\n[LightGBM] [Info] Start training from score -0.548697\nFold 3 AUCs: 0.8904685701370813 0.8912663115616004 0.8952837488675519\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 4394, number of negative: 7606\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3172\n[LightGBM] [Info] Number of data points in the train set: 12000, number of used features: 33\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.366167 -> initscore=-0.548697\n[LightGBM] [Info] Start training from score -0.548697\nFold 4 AUCs: 0.8873815119354758 0.8845695452395044 0.8857711851583704\nENSEMBLE AUC: 0.8872182538965157\nsubmission.csv saved!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}